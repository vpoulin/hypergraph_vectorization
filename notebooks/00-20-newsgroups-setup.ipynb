{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd1b4c4-2b53-41c2-9edc-7dc06f3ff5ae",
   "metadata": {},
   "source": [
    "## Preparing the 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fab39-c678-4247-b854-938985d091e9",
   "metadata": {},
   "source": [
    "We will need some data to work with. For the purposes of this demo we will make use of the 20 newsgroups dataset. Even though 20 newsgroups is a toy dataset, it offers enough complications to show how we can piece together embeddings using ``vectorizers``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5bfc53a-bcae-4a52-b09d-3d2c8d662917",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04d8d90-48f5-4971-834f-539a2b2df938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src import paths\n",
    "# from src.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8417332-5659-4467-bec3-472fd2462d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "from sklearn.utils import Bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaca0aef-f420-4c67-9078-a66d7470be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "opts={\"subset\":\"all\", \"remove\":\"('headers', 'footers', 'quotes')\"}\n",
    "ds_in = fetch_20newsgroups(**opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "115835a3-ecb6-4440-accc-527f5d841202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality\n"
     ]
    }
   ],
   "source": [
    "# ds_in = Dataset.load('20_newsgroups')\n",
    "print(ds_in.DESCR[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67880107-ed58-48b2-ab49-edfabed84076",
   "metadata": {},
   "source": [
    "First, we will do a little pruning: the 20 newsgroups dataset contains some extremely short posts (once the headers, footers and quotes are removed) that will muddy up the results. We will prune out any post less than 200 characters (200 is chosen somewhat arbitrarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405c7863-977b-4c4c-9f9c-a2137738624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_limit = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9293e5fd-10a2-4451-8186-937a16145576",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_enough = [len(t) > prune_limit for t in ds_in.data]\n",
    "targets = np.array(ds_in.target)[long_enough]\n",
    "news_data = [t for t in ds_in.data if len(t) > prune_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc7ac802-9c5a-4081-9c5a-e26aa9f49807",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Bunch(data = new_data, \n",
    "           filenames = ds_in.filenames, \n",
    "           target_names = ds_in.target_names, \n",
    "           target = targets,\n",
    "          DESCR = ds_in.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb3f1d-70fb-438e-a9ae-e30d7c7dd5ec",
   "metadata": {},
   "source": [
    "For each newsgroup post, the target data is the name of the newsgroup the post was sent to. There are broad groups that the specific newsgroups can be classified into, such as religion, politics, computing, sport and science. While some of broad groups can be gleaned programmatically from the newsgroup name (with its dotted hierarchy), some groups (like alt.atheism being in the religion topic) require more care. We will hand curate the newsgroups into 6 broad categories:\n",
    "* religion\n",
    "* politics\n",
    "* sport\n",
    "* comp\n",
    "* sci\n",
    "* misc\n",
    "\n",
    "Using these broad categories, we will create a custom color palette for the data when visualizing results such that different newsgroups in the same category can have similar colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ac872d0-55d2-446e-8dad-7819e4b5d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion = (\"alt.atheism\", \"talk.religion.misc\", \"soc.religion.christian\")\n",
    "politics = (\"talk.politics.misc\", \"talk.politics.mideast\", \"talk.politics.guns\")\n",
    "sport = (\"rec.sport.baseball\", \"rec.sport.hockey\")\n",
    "comp = (\n",
    "    \"comp.graphics\",\n",
    "    \"comp.os.ms-windows.misc\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"comp.sys.mac.hardware\",\n",
    "    \"comp.windows.x\",\n",
    ")\n",
    "sci = (\n",
    "    \"sci.crypt\",\n",
    "    \"sci.electronics\",\n",
    "    \"sci.med\",\n",
    "    \"sci.space\",\n",
    ")\n",
    "misc = (\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"rec.motorcycles\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1208a0a-b7ab-4a9f-95f7-36cf78e7d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_key = {}\n",
    "for l, c in zip(religion, sns.color_palette(\"Blues\", 4)[1:]):\n",
    "    color_key[l] = matplotlib.colors.rgb2hex(c)\n",
    "for l, c in zip(politics, sns.color_palette(\"Purples\", 4)[1:]):\n",
    "    color_key[l] = matplotlib.colors.rgb2hex(c)\n",
    "for l, c in zip(comp, sns.color_palette(\"YlOrRd\", 5)):\n",
    "    color_key[l] = matplotlib.colors.rgb2hex(c)\n",
    "for l, c in zip(sci, sns.color_palette(\"light:teal\", 5)[1:]):\n",
    "    color_key[l] = matplotlib.colors.rgb2hex(c)\n",
    "for l, c in zip(sport, sns.color_palette(\"light:#660033\", 4)[1:3]):\n",
    "    color_key[l] = matplotlib.colors.rgb2hex(c)\n",
    "for l, c in zip(misc, sns.color_palette(\"YlGn\", 4)[1:]):\n",
    "    color_key[l] = matplotlib.colors.rgb2hex(c)\n",
    "color_key[\"word\"] = \"#777777bb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472d625-2fc1-4afc-a1c5-17f6ab2fc692",
   "metadata": {},
   "source": [
    "With a dataset and a carefully designed color palette we are in good shape to do some analysis of different embedding methods using UMAP to obtain visualizations of the embeddings. \n",
    "\n",
    "## Save this Dataset\n",
    "Let's save this as a dataset for easy re-use in our other notebooks, and add the color palette to the metadata of the dataset. \n",
    "\n",
    "Note: This Dataset has already been added to the catalog and the following cells do not need to be run again. They are included here as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "458bd7ef-3727-4e02-9b16-5c223f3891fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_as_transformer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.helpers import notebook_as_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fd76335-d49d-4982-852f-4f98597684ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m new_target \u001b[38;5;241m=\u001b[39m targets\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# new_metadata = ds_in.metadata.copy()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# new_metadata['color_key'] = color_key\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# added_descr_txt = f\"\"\"\\n This dataset is a subselection of the {ds_in.name} Dataset where we have pruned out any post less than {prune_limit} \\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# characters ({prune_limit} is chosen somewhat arbitrarily). A custom `color_key` can be found in the metadata.\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# new_metadata['descr'] += added_descr_txt\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m new_ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m(dataset_name\u001b[38;5;241m=\u001b[39mnew_dataset_name, data\u001b[38;5;241m=\u001b[39mnew_data, target\u001b[38;5;241m=\u001b[39mnew_target,\n\u001b[1;32m     11\u001b[0m                  metadata\u001b[38;5;241m=\u001b[39mnew_metadata)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# new_dataset_name = f'{ds_in.name}_pruned'\n",
    "new_data = news_data\n",
    "new_target = targets\n",
    "# new_metadata = ds_in.metadata.copy()\n",
    "# new_metadata['color_key'] = color_key\n",
    "# added_descr_txt = f\"\"\"\\n This dataset is a subselection of the {ds_in.name} Dataset where we have pruned out any post less than {prune_limit} \\\n",
    "# characters ({prune_limit} is chosen somewhat arbitrarily). A custom `color_key` can be found in the metadata.\"\"\"\n",
    "# new_metadata['descr'] += added_descr_txt\n",
    "\n",
    "new_ds = Dataset(dataset_name=new_dataset_name, data=new_data, target=new_target,\n",
    "                 metadata=new_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a279bef-aabb-4bfc-8257-b38259ca253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 14:06:53,120 - datasets - WARNING - Overwrite_catalog=True but generate=False. Not overwriting Dataset catalog entry for '20_newsgroups_pruned'\n"
     ]
    }
   ],
   "source": [
    "# Due to various design choiced in Jupyter, we need to specify this name manually.\n",
    "nbname = '00-20-newsgroups-setup.ipynb'\n",
    "dsdict = notebook_as_transformer(notebook_name=nbname,\n",
    "                                 input_datasets=[ds_in],\n",
    "                                 output_datasets=[new_ds],\n",
    "                                 overwrite_catalog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31643cb4-25a7-4cd1-9882-3555014981b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TNT env",
   "language": "python",
   "name": "tnt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
